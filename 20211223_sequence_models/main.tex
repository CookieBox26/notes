\documentclass[b5paper,xelatex,ja=standard,10pt]{bxjsarticle}
\usepackage{mystyle}  % export TEXINPUTS="./;../sty/;"
\graphicspath{{../images/}}

\usepackage{listings}
\lstset{  % グローバル設定
  columns=fixed,  % 等幅
  basewidth=0.5em,  % 字間詰め
  lineskip=-3pt,  % 行間詰め
  % フォント設定
  basicstyle={\ttfamily\small\color{DarkGray}},  % 全体設定
  keywordstyle=[1]{\color{RoyalBlue}},  % kewords[1]の設定 (Python だと予約語)
  keywordstyle=[2]{\color{VioletRed}},  % kewords[2]の設定 (Python だと組み込み関数)
  stringstyle={\color{FireBrick}},  % 文字列リテラルの設定
  commentstyle={\color{SeaGreen}},  % コメントの設定
}

\renewcommand{\footnoterule}{%
  \kern -5pt
  \color{DarkGray}
  \hrule width \textwidth height 0.5pt
  \kern 5pt
}

%\makeatletter
%\renewcommand*\l@section{\@dottedtocline{1}{0.0em}{4.0em}}
%\makeatother

\usepackage{eso-pic}

\newcommand\BackgroundPic{%
\put(0,0){%
\parbox[b][\paperheight]{\paperwidth}{%
\vfill
\centering
\includegraphics[width=1.0\paperwidth,height=1.0\paperheight,%
keepaspectratio]{cover.png}%
\vfill
}}}

\AddToShipoutPictureBG{
  \AtPageUpperLeft{
    \raisebox{-2.2\baselineskip}{\makebox[\paperwidth]{\begin{minipage}{14cm}\centering
      {\addfontfeatures{Color=PaleVioletRed} \textbf{\small DRAFT 2022-01-13 \\ \url{https://github.com/CookieBox26/notes/tree/main/20211223_sequence_models} } }
    \end{minipage}}}
  }
}



\begin{document}


% タイトル画像
\AddToShipoutPicture*{
  \BackgroundPic
  \AtPageUpperLeft{
    \raisebox{-2.7\baselineskip}{\makebox[\paperwidth]{\begin{minipage}{14cm}\centering
      {\addfontfeatures{Color=PaleVioletRed} \textbf{\small DRAFT 2022-01-13 \\ \url{https://github.com/CookieBox26/notes/tree/main/20211223_sequence_models} } }
    \end{minipage}}}
  }
}
\begin{titlepage}
\ 
\end{titlepage}


% 地の文の文字色をグレーに変更する
\addfontfeatures{Color=DarkGray}
\addCJKfontfeatures{Color=DarkGray}


% 目次
\begin{spacing}{1.0}
\tableofcontents
\end{spacing}


% まえがき
\newpage
\section*{まえがき}
\addcontentsline{toc}{section}{まえがき}
\vspace{3pt}

本書は機械学習の国際会議 \, {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021}{ NeurIPS 2021}} で発表された論文から、(時)系列データを処理するためのニューラルネットワークモデル――長いのでニューラル系列モデルとよびます――に関連する研究を私見で眺めたものです。網羅的でも排他的でもありません。

本書の内容は著者の理解であることにご留意ください。著者の誤りは著者に帰属します。

本書の内容についてお気付きの点がありましたら、大変お手数ですが、この原稿があるリポジトリの Issues、または著者ブログのコメント欄までお知らせください。著者ブログへのコメントはただちには公開されません。非公開希望の方はその旨をお知らせください。非公開希望であって返信が必要な場合はご連絡先の明記をお願いいたします。

\begin{description}
   \item[　リポジトリ] https://github.com/CookieBox26/notes/
   \item[　著者ブログ] https://cookie-box.hatenablog.com/
\end{description}

\vspace{6pt}
%\subsubsection*{登場人物紹介}
\centerline{\textbf{登場人物紹介}}

\vspace{3pt}
\begin{SERIFU}[colback=White, colbacktitle=PaleIris2, top=-1pt, bottom=0pt, left=18pt, right=10pt]{kazusa_3.png}
\small
この人はベイズ統計部の部長です。1年生です。とある目的のためにベイズ統計部を立ち上げ、統計や機械学習を勉強しています。ベイズ統計部には部長と副部長しかいません。三姉妹の末っ子です。
\end{SERIFU}
\begin{SERIFU}[colback=White, colbacktitle=PaleGold2, top=-1pt, bottom=0pt, left=18pt, right=10pt]{takumi_2.png}
\small
この人はベイズ統計部の副部長です。2年生です。海外からの編入生でラクロス部に入部しようとしていましたが、部長に勧誘されてベイズ統計部に入部しました。数学が得意ですがなぜか著者を超える数学力が出せません。
\end{SERIFU}
\vspace{4pt}


\section*{NeurIPS 2021 のニューラル系列モデルを眺める}
\addcontentsline{toc}{section}{NeurIPS 2021 のニューラル系列モデルを眺める}
\vspace{5pt}

\begin{SERIFU}[colback=PaleIris, colbacktitle=PaleIris2]{kazusa_3.png}
(時)系列データを処理するためのニューラルネットワークモデルの動向を知るために、NeurIPS 2021 で発表された研究をみていきましょう。NeurIPS 2021 で発表された論文の総数は……2334本\footnote{2021年11月28日時の https://proceedings.neurips.cc/paper/2021 のリンク数に基づく。}！？
\end{SERIFU}

\begin{SERIFU}[colback=PaleIris, colbacktitle=PaleIris2]{kazusa_1.png}
あ、あまりに多いので機械的に絞り込みましょう。さしあたりタイトルに time series, sequential, rnn, recurrent, transformer, attention, state space のいずれかを含む論文は……それでも 155本……。ただこの絞り込みだと画像認識, GAN, 強化学習の研究も多そうですね。それらも興味深いですが、(時)系列に関する研究を優先するべく断腸の思いでとばしていきましょう……。
\end{SERIFU}

\begin{SERIFU}[colback=PaleIris, colbacktitle=PaleIris2]{kazusa_2.png}
とばしても 39 本……多いですね……こう多いと何が何だかわかりません。モデルの切り口でグループ分けして、アブストラクトからの理解で整理してみましょう。
\end{SERIFU}

\begin{SERIFU}[colback=PaleIris, colbacktitle=PaleIris2]{kazusa_3.png}
まず最大勢力は Transformer ですね、検索語に含めたのでヒットするのは当然ですが、多くを占めます……\textbf{「セルフアテンションの計算量に対処する」}は昨年以前から引き続き人気(?)なテーマであるようです。このサブグループは後で改めてメモしましょう。
\end{SERIFU}

\vspace{1pt}
\begin{PROP}[colback=White, left=0pt]{グループ「Transformer」}
\begin{itemize}
  \item Transformerの性質を理解する。
    \begin{itemize}
    \item 行列分解でパラメータを10倍削減しても性能が出ると示す \cite{AliakbarPanahi2021}。
    \item セルフアテンションを生物学的な記憶モデルと解釈する \cite{TrentonBricken2021}。
  \end{itemize}
  \item Transformerを工夫する。
  \begin{itemize}
    \item 機械的にプレ処理(トレンド-季節性分解)をする \cite{HaixuWu2021}。
    \item グリッド分割をさらにグリッド分割する(Vision Transformer) \cite{KaiHan2021}。
    \item \textbf{セルフアテンションの計算量に対処する} \cite{YifanChen2021} \cite{XuezheMa2021} \cite{SubhabrataDutta2021} \cite{SebastianJaszczur2021} \cite{BeidiChen2021} \cite{ChenZhu2021} \cite{HongyuRen2021} \cite{ShengjieLuo2021}。
    \item ヘッド間で $Q, K$ の分布を一致させる正則化をする \cite{ShujianZhang2021}。
    \item Transformerのアーキテクチャ自体を再考する。
    \begin{itemize}
      \item 言語処理に適した構造を探索する \cite{DavidSo2021}
      \item セルフアテンションの代わりにゲート付MLPにする \cite{HanxiaoLiu2021}。
      \item 状態空間モデルと組み合わせて時系列の長期予測等をする \cite{BinhTang2021}。
    \end{itemize}
  \end{itemize}
\end{itemize}
\end{PROP}
\vspace{1pt}

\begin{SERIFU}[colback=PaleIris, colbacktitle=PaleIris2]{kazusa_3.png}
次は RNN ですね。\textbf{「RNN を理論的に理解する」}という研究が割にみられるように感じられます。理論解析が進めばどのような系列データにどのようなニューラルアーキテクチャを用いるべきかにつながるのでしょうか……??
\end{SERIFU}

\vspace{1pt}
\begin{PROP}[colback=White, left=0pt]{グループ「RNN」}
\begin{itemize}
  \item \textbf{RNN を理論的に理解する}。
  \begin{itemize}
    \item RNN がある再生核ヒルベルト空間におけるカーネル法であると示す \cite{AdelineFermanian2021}。
    \item スイッチング線形動的システムで RNN をリバースエンジニアリングする \cite{JimmySmith2021}。
    \item RNN が学習できると保証される関数の制約を撤廃する \cite{LifuWang2021} \cite{AbhishekPanigrahi2021}。
  \end{itemize}
  \item RNN を工夫する。
  \begin{itemize}
    \item 訓練時に隠れ状態にノイズ添加してロバストにする \cite{SoonHoeLim2021}。
    \item RNN 自体が時間変化できるようにする \cite{AstonZhang2021}。
  \end{itemize}
\end{itemize}
\end{PROP}
\vspace{1pt}

\begin{SERIFU}[colback=PaleIris, colbacktitle=PaleIris2]{kazusa_2.png}
後はその他とでもしましょうか。
\end{SERIFU}

\vspace{1pt}
\begin{PROP}[colback=White, left=0pt]{その他}
\begin{itemize}
  \item 機械的に汎用的なプレ処理(成分クラスタリング)をする \cite{ZhiboZhu2021}。
  \item 時系列を生成する混合分布を推定するためのコアセットを構築する \cite{LingxiaoHuang2021}。
  \item 微分方程式で記述されるシステムをニューラルネットで実現する。
  \begin{itemize}
    \item 線形時不変連続時間システムをニューラルネットで実現する \cite{AlbertGu2021}。
    \item 連立微分方程式システムをベイズフィルタで解く \cite{JonathanSchmidt2021}。
  \end{itemize}
  \item 系列モデルを新しい用途に活用する。
  \begin{itemize}
    \item Transformer を活用してガウス過程モデル適用時のカーネルを同定する \cite{FergusSimpson2021}。
  \end{itemize}
\end{itemize}
\end{PROP}
\vspace{1pt}

\begin{SERIFU}[colback=PaleIris, colbacktitle=PaleIris2]{kazusa_2.png}
最後に\textbf{「セルフアテンションの計算量に対処する」}を改めてみましょう。Transformer の計算量が取り沙汰されるのは $\mathrm{Softmax} \left( Q K ^\top / \sqrt{d} \right) \in \mathbb{R}^{N \times N}$ を求めるのに系列長 $N$ に対して $\mathcal{O}(N^2)$ の計算量がかかるためですが、$\mathcal{O}(N^2)$ を回避するために、以下のようなアプローチが取られているようです。スパース化、低ランク近似自体はこれまでも計算量削減の基本路線であったと思いますが、新たな切り口を導入しているのと、その他の独自路線アプローチもみられるのではないでしょうか。
\end{SERIFU}

\vspace{1pt}
\begin{PROP}[colback=White, left=0pt, borderline={0.5mm}{0mm}{white,dashed}]{サブグループ「セルフアテンションの計算量に対処する」}
\begin{itemize}
  \item $Q K ^\top$ の成分を間引く(スパースにする) 。
  \begin{itemize}
    \item どの成分か不要なのか自体を学習する \cite{SebastianJaszczur2021}。
  \end{itemize}
  \item $Q K ^\top$ を低ランク近似する(行列分解する)。
  \begin{itemize}
    \item カーネル法の計算量削減のアプローチを応用する \cite{YifanChen2021}。
  \end{itemize}
  \item スパース化と低ランク近似を統合する \cite{BeidiChen2021}。
  \vspace{5pt}
  \item $Q K ^\top$ の計算箇所でだけ入力系列を短い系列に射影する \cite{XuezheMa2021}。
  \begin{itemize}
    \item 短距離依存性はそのまま計算し、長距離依存性は短い系列に射影する \cite{ChenZhu2021}。
  \end{itemize}
  \vspace{5pt}
  \item 長距離依存性については重み付き期待値に対してアテンションする \cite{HongyuRen2021}。
  \vspace{5pt}
  \item 最初のセルフアテンション層では $Q K ^\top$ を計算するが、2番目以降ではそれを時間発展させる \cite{SubhabrataDutta2021}。
  \vspace{5pt}
  \item アテンションの計算に高速フーリエ変換を応用する \cite{ShengjieLuo2021}。
\end{itemize}
\end{PROP}
\vspace{1pt}


\section*{それぞれのお話}
\addcontentsline{toc}{section}{それぞれのお話}


\newcommand*{\mysubsectiontitle}{RNN、CNN、連続時間モデルを結合したい}
\subsection*{\cite{AlbertGu2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{AlbertGu2021} \, \mysubsectiontitle}
\vspace{3pt}
ほげ。
\begin{SERIFU}[colback=PaleGold, colbacktitle=PaleGold2]{takumi_2.png}
\end{SERIFU}

\renewcommand*{\mysubsectiontitle}{ほげ}
\subsection*{\cite{AliakbarPanahi2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{AliakbarPanahi2021} \, \mysubsectiontitle}
\vspace{3pt}
ほげ。


\renewcommand*{\mysubsectiontitle}{Skyformer――セルフアテンションの Nyström 近似}
\subsection*{\cite{YifanChen2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{YifanChen2021} \, \mysubsectiontitle}
\vspace{3pt}

\begin{SERIFU}[colback=PaleIris, colbacktitle=PaleIris2]{kazusa_1.png}
これまでのセルフアテンションの計算量削減には往々にして近似誤差の理論保証がないというようにいっていますね。だから手法間の比較もできなくなっているし、ハイパーパラメータによる計算量削減度合いの調整もできなくなっていると――
\end{SERIFU}

\begin{SERIFU}[colback=PaleIris, colbacktitle=PaleIris2]{kazusa_1.png}
Nyström 近似?
\end{SERIFU}


\renewcommand*{\mysubsectiontitle}{ほげ}
\subsection*{\cite{XuezheMa2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{XuezheMa2021} \, \mysubsectiontitle}
\vspace{3pt}
ほげ。


\renewcommand*{\mysubsectiontitle}{ほげ}
\subsection*{\cite{AdelineFermanian2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{AdelineFermanian2021} \, \mysubsectiontitle}
\vspace{3pt}
ほげ。


\renewcommand*{\mysubsectiontitle}{ほげ}
\subsection*{\cite{SoonHoeLim2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{SoonHoeLim2021} \, \mysubsectiontitle}
\vspace{3pt}
ほげ。


\renewcommand*{\mysubsectiontitle}{ほげ}
\subsection*{\cite{SubhabrataDutta2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{SubhabrataDutta2021} \, \mysubsectiontitle}
\vspace{3pt}
ほげ。


\renewcommand*{\mysubsectiontitle}{ほげ}
\subsection*{\cite{DavidSo2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{DavidSo2021} \, \mysubsectiontitle}
\vspace{3pt}
ほげ。


\renewcommand*{\mysubsectiontitle}{ほげ}
\subsection*{\cite{AstonZhang2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{AstonZhang2021} \, \mysubsectiontitle}
\vspace{3pt}
ほげ。


\renewcommand*{\mysubsectiontitle}{ほげ}
\subsection*{\cite{HanxiaoLiu2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{HanxiaoLiu2021} \, \mysubsectiontitle}
\vspace{3pt}
ほげ。


\renewcommand*{\mysubsectiontitle}{ほげ}
\subsection*{\cite{SebastianJaszczur2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{SebastianJaszczur2021} \, \mysubsectiontitle}
\vspace{3pt}
ほげ。


\renewcommand*{\mysubsectiontitle}{ほげ}
\subsection*{\cite{FergusSimpson2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{FergusSimpson2021} \, \mysubsectiontitle}
\vspace{3pt}
ほげ。


\renewcommand*{\mysubsectiontitle}{ほげ}
\subsection*{\cite{JonathanSchmidt2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{JonathanSchmidt2021} \, \mysubsectiontitle}
\vspace{3pt}
ほげ。


\renewcommand*{\mysubsectiontitle}{ほげ}
\subsection*{\cite{ZhiboZhu2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{ZhiboZhu2021} \, \mysubsectiontitle}
\vspace{3pt}
ほげ。


\renewcommand*{\mysubsectiontitle}{ほげ}
\subsection*{\cite{ShujianZhang2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{ShujianZhang2021} \, \mysubsectiontitle}
\vspace{3pt}
ほげ。


\renewcommand*{\mysubsectiontitle}{ほげ}
\subsection*{\cite{TrentonBricken2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{TrentonBricken2021} \, \mysubsectiontitle}
\vspace{3pt}
ほげ。


\renewcommand*{\mysubsectiontitle}{ほげ}
\subsection*{\cite{KaiHan2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{KaiHan2021} \, \mysubsectiontitle}
\vspace{3pt}
ほげ。


\renewcommand*{\mysubsectiontitle}{ほげ}
\subsection*{\cite{JimmySmith2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{JimmySmith2021} \, \mysubsectiontitle}
\vspace{3pt}
ほげ。


\renewcommand*{\mysubsectiontitle}{ほげ}
\subsection*{\cite{BeidiChen2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{BeidiChen2021} \, \mysubsectiontitle}
\vspace{3pt}
ほげ。


\renewcommand*{\mysubsectiontitle}{ほげ}
\subsection*{\cite{ChenZhu2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{ChenZhu2021} \, \mysubsectiontitle}
\vspace{3pt}
ほげ。


\renewcommand*{\mysubsectiontitle}{ほげ}
\subsection*{\cite{LifuWang2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{LifuWang2021} \, \mysubsectiontitle}
\vspace{3pt}
ほげ。


\renewcommand*{\mysubsectiontitle}{ほげ}
\subsection*{\cite{AbhishekPanigrahi2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{AbhishekPanigrahi2021} \, \mysubsectiontitle}
\vspace{3pt}
ほげ。


\renewcommand*{\mysubsectiontitle}{ほげ}
\subsection*{\cite{HaixuWu2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{HaixuWu2021} \, \mysubsectiontitle}
\vspace{3pt}
ほげ。


\renewcommand*{\mysubsectiontitle}{ほげ}
\subsection*{\cite{HongyuRen2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{HongyuRen2021} \, \mysubsectiontitle}
\vspace{3pt}
ほげ。


\renewcommand*{\mysubsectiontitle}{ほげ}
\subsection*{\cite{ShengjieLuo2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{ShengjieLuo2021} \, \mysubsectiontitle}
\vspace{3pt}

\begin{SERIFU}[colback=PaleIris, colbacktitle=PaleIris2]{kazusa_1.png}
アブストラクトは、「セルフアテンションの計算量を削減する既存研究の多くは『内積をとってからソフトマックスする』方式にしか対応できない」と主張しているようにみえます。そして、それだと「相対位置エンコーディング(RPE)に対応できない」と……これまでに提案されているセルフアテンション計算量削減ってそんなに制約があったんですか??
\end{SERIFU}


\renewcommand*{\mysubsectiontitle}{ほげ}
\subsection*{\cite{LingxiaoHuang2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{LingxiaoHuang2021} \, \mysubsectiontitle}
\vspace{3pt}
ほげ。


\renewcommand*{\mysubsectiontitle}{ほげ}
\subsection*{\cite{BinhTang2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{BinhTang2021} \, \mysubsectiontitle}
\vspace{3pt}
ほげ。


\renewcommand*{\mysubsectiontitle}{ほげ}
\subsection*{\cite{AnupSarma2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{AnupSarma2021} \, \mysubsectiontitle}
\vspace{3pt}
ほげ。


\renewcommand*{\mysubsectiontitle}{ほげ}
\subsection*{\cite{SebastianZeng2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{SebastianZeng2021} \, \mysubsectiontitle}
\vspace{3pt}
ほげ。


\renewcommand*{\mysubsectiontitle}{ほげ}
\subsection*{\cite{ShuhaoCao2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{ShuhaoCao2021} \, \mysubsectiontitle}
\vspace{3pt}
ほげ。


\renewcommand*{\mysubsectiontitle}{ほげ}
\subsection*{\cite{EliaTurner2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{EliaTurner2021} \, \mysubsectiontitle}
\vspace{3pt}
ほげ。


\renewcommand*{\mysubsectiontitle}{ほげ}
\subsection*{\cite{ZimingZhang2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{ZimingZhang2021} \, \mysubsectiontitle}
\vspace{3pt}
ほげ。


\renewcommand*{\mysubsectiontitle}{ほげ}
\subsection*{\cite{QuentinRebjock2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{QuentinRebjock2021} \, \mysubsectiontitle}
\vspace{3pt}
ほげ。


\renewcommand*{\mysubsectiontitle}{ほげ}
\subsection*{\cite{YutongBai2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{YutongBai2021} \, \mysubsectiontitle}
\vspace{3pt}
ほげ。


\renewcommand*{\mysubsectiontitle}{ほげ}
\subsection*{\cite{StephenChung2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{StephenChung2021} \, \mysubsectiontitle}
\vspace{3pt}
ほげ。


\renewcommand*{\mysubsectiontitle}{ほげ}
\subsection*{\cite{JannikKossen2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{JannikKossen2021} \, \mysubsectiontitle}
\vspace{3pt}
ほげ。


\renewcommand*{\mysubsectiontitle}{ほげ}
\subsection*{\cite{TanNguyen2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{TanNguyen2021} \, \mysubsectiontitle}
\vspace{3pt}
ほげ。


\renewcommand*{\mysubsectiontitle}{ほげ}
\subsection*{\cite{FanKengSun2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{FanKengSun2021} \, \mysubsectiontitle}
\vspace{3pt}
ほげ。


\renewcommand*{\mysubsectiontitle}{ほげ}
\subsection*{\cite{AbdulFatirAnsari2021} \, \mysubsectiontitle}
\addcontentsline{toc}{subsection}{\cite{AbdulFatirAnsari2021} \, \mysubsectiontitle}
\vspace{3pt}
ほげ。


\section*{結び――NeurIPS 2021 にみる最近のニューラル系列モデルへの発見・工夫・理解}
\addcontentsline{toc}{section}{結び――NeurIPS 2021 にみる最近のニューラル系列モデルへの発見・工夫・理解}


\section*{Appendix}
\addcontentsline{toc}{section}{Appendix}

\renewcommand*{\mysubsectiontitle}{公開コードを動かす――\cite{YifanChen2021} Skyformer}
\subsection*{\mysubsectiontitle}
\addcontentsline{toc}{subsection}{\mysubsectiontitle}
\vspace{3pt}

\begin{SERIFU}[colback=PaleIris, colbacktitle=PaleIris2]{kazusa_3.png}
Skyformer \cite{YifanChen2021}を動かしてみましょう。
\begin{CODE}[boxrule=0pt,frame hidden]
\begin{lstlisting}[language=python]
from models.model_LRA import ModelForSC, ModelForSCDual
from config import Config

model_config = Config["lra-text"]["model"]
model_config["mixed_precision"] = True
model_config["attn_type"] = "softmax"
model = ModelForSC(model_config)
print(model)
\end{lstlisting}
\end{CODE}

\begin{CODE}[boxrule=0pt,frame hidden,colback=SlateGray2]
\begin{lstlisting}[basicstyle={\ttfamily\small\color{White}}]
ModelForSC(
  (model): Model(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(512, 64)
      (position_embeddings): Embedding(4000, 64)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer_0): TransformerLayer(
      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (mha): Attention(
        (W_q): Linear(in_features=64, out_features=64, bias=True)
        (W_k): Linear(in_features=64, out_features=64, bias=True)
        (W_v): Linear(in_features=64, out_features=64, bias=True)
        (attn): SoftmaxAttention(
          (drop_attn): Dropout(p=0.1, inplace=False)
        )
# 以下省略
\end{lstlisting}
\end{CODE}



これに Long Range Arena のデータを渡せばよいですね。
\end{SERIFU}


% 参考文献
\clearpage
\addcontentsline{toc}{section}{参考文献}
\begin{thebibliography}{99}
    \bibitem{AlbertGu2021} Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, Christopher Ré. Combining Recurrent, Convolutional, and Continuous-time Models with Linear State Space Layers. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/05546b0e38ab9175cd905eebcc6ebb76-Abstract.html}{In NeurIPS 2021}}.
    \bibitem{AliakbarPanahi2021} Aliakbar Panahi, Seyran Saeedi, Tom Arodz. Shapeshifter: a Parameter-efficient Transformer using Factorized Reshaped Matrices. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/09def3ebbc44ff3426b28fcd88c83554-Abstract.html}{In NeurIPS 2021}}.
    \bibitem{YifanChen2021} Yifan Chen, Qi Zeng, Heng Ji, Yun Yang. Skyformer: Remodel Self-Attention with Gaussian Kernel and Nyström method. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/10a7cdd970fe135cf4f7bb55c0e3b59f-Abstract.html}{In NeurIPS 2021}}.
    \bibitem{XuezheMa2021} Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, Luke Zettlemoyer. Luna: Linear Unified Nested Attention. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/14319d9cfc6123106878dc20b94fbaf3-Abstract.html}{In NeurIPS 2021}}.
    \bibitem{AdelineFermanian2021} Adeline Fermanian, Pierre Marion, Jean-Philippe Vert, Gérard Biau. Framing RNN as a kernel method: A neural ODE approach. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/18a9042b3fc5b02fe3d57fea87d6992f-Abstract.html}{In NeurIPS 2021}}.

    \bibitem{SoonHoeLim2021} Soon Hoe Lim, N. Benjamin Erichson, Liam Hodgkinson, Michael W. Mahoney. Noisy Recurrent Neural Networks. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/29301521774ff3cbd26652b2d5c95996-Abstract.html}{In NeurIPS 2021}}.
    \bibitem{SubhabrataDutta2021} Subhabrata Dutta, Tanya Gautam, Soumen Chakrabarti, Tanmoy Chakraborty. Redesigning the Transformer Architecture with Insights from Multi-particle Dynamical Systems. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/2bd388f731f26312bfc0fe30da009595-Abstract.html}{In NeurIPS 2021}}.
    \bibitem{DavidSo2021} David So, Wojciech Mańke, Hanxiao Liu, Zihang Dai, Noam Shazeer, Quoc Le. Searching for Efficient Transformers for Language Modeling. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/2f3c6a4cd8af177f6456e7e51a916ff3-Abstract.html}{In NeurIPS 2021}}.
    \bibitem{AstonZhang2021} Aston Zhang, Yi Tay, Yikang Shen, Alvin Chan Guo Wei, SHUAI ZHANG. Self-Instantiated Recurrent Units with Dynamic Soft Recursion. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/3341f6f048384ec73a7ba2e77d2db48b-Abstract.html}{In NeurIPS 2021}}.
    \bibitem{HanxiaoLiu2021} Hanxiao Liu, Zihang Dai, David So, Quoc Le. Pay Attention to MLPs. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/4cc05b35c2f937c5bd9e7d41d3686fff-Abstract.html}{In NeurIPS 2021}}.

    \bibitem{SebastianJaszczur2021} Sebastian Jaszczur, Aakanksha Chowdhery, Afroz Mohiuddin, Łukasz Kaiser, Wojciech Gajewski, Henryk Michalewski, Jonni Kanerva. Sparse is Enough in Scaling Transformers. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/51f15efdd170e6043fa02a74882f0470-Abstract.html}{In NeurIPS 2021}}.
    \bibitem{FergusSimpson2021} Fergus Simpson, Ian Davies, Vidhi Lalchand, Alessandro Vullo, Nicolas Durrande, Carl Edward Rasmussen. Kernel Identification Through Transformers. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/56c3b2c6ea3a83aaeeff35eeb45d700d-Abstract.html}{In NeurIPS 2021}}.
    \bibitem{JonathanSchmidt2021} Jonathan Schmidt, Nicholas Krämer, Philipp Hennig. A Probabilistic State Space Model for Joint Inference from Differential Equations and Data. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/6734fa703f6633ab896eecbdfad8953a-Abstract.html}{In NeurIPS 2021}}.
    \bibitem{ZhiboZhu2021} Zhibo Zhu, Ziqi Liu, Ge Jin, Zhiqiang Zhang, Lei Chen, Jun Zhou, Jianyong Zhou. MixSeq: Connecting Macroscopic Time Series Forecasting with Microscopic Time Series Data. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/6b5754d737784b51ec5075c0dc437bf0-Abstract.html}{In NeurIPS 2021}}.
    \bibitem{ShujianZhang2021} Shujian Zhang, Xinjie Fan, Huangjie Zheng, Korawat Tanwisuth, Mingyuan Zhou. Alignment Attention by Matching Key and Query Distributions. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/6fd6b030c6afec018415662d0db43f9d-Abstract.html}{In NeurIPS 2021}}.

    \bibitem{TrentonBricken2021} Trenton Bricken, Cengiz Pehlevan. Attention Approximates Sparse Distributed Memory. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/8171ac2c5544a5cb54ac0f38bf477af4-Abstract.html}{In NeurIPS 2021}}.
    \bibitem{KaiHan2021} Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing XU, Yunhe Wang. Transformer in Transformer. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/854d9fca60b4bd07f9bb215d59ef5561-Abstract.html}{In NeurIPS 2021}}.
    \bibitem{JimmySmith2021} Jimmy Smith, Scott Linderman, David Sussillo. Reverse engineering recurrent neural networks with Jacobian switching linear dynamical systems. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/8b77b4b5156dc11dec152c6c71481565-Abstract.html}{In NeurIPS 2021}}.
    \bibitem{BeidiChen2021} Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, Christopher Ré. Scatterbrain: Unifying Sparse and Low-rank Attention. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/9185f3ec501c674c7c788464a36e7fb3-Abstract.html}{In NeurIPS 2021}}.
    \bibitem{ChenZhu2021} Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, Bryan Catanzaro. Long-Short Transformer: Efficient Transformers for Language and Vision. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/9425be43ba92c2b4454ca7bf602efad8-Abstract.html}{In NeurIPS 2021}}.

    \bibitem{LifuWang2021} Lifu Wang, Bo Shen, Bo Hu, Xing Cao. On the Provable Generalization of Recurrent Neural Networks. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/a928731e103dfc64c0027fa84709689e-Abstract.html}{In NeurIPS 2021}}.
    \bibitem{AbhishekPanigrahi2021} Abhishek Panigrahi, Navin Goyal. Learning and Generalization in RNNs. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/b04c387c8384ca083a71b8da516f65f6-Abstract.html}{In NeurIPS 2021}}.
    \bibitem{HaixuWu2021} Haixu Wu, Jiehui Xu, Jianmin Wang, Mingsheng Long. Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/bcc0d400288793e8bdcd7c19a8ac0c2b-Abstract.html}{In NeurIPS 2021}}.
    \bibitem{HongyuRen2021} Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale Schuurmans, Bo Dai. Combiner: Full Attention Transformer with Sparse Computation Cost. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/bd4a6d0563e0604510989eb8f9ff71f5-Abstract.html}{In NeurIPS 2021}}.
    \bibitem{ShengjieLuo2021} Shengjie Luo, Shanda Li, Tianle Cai, Di He, Dinglan Peng, Shuxin Zheng, Guolin Ke, Liwei Wang, Tie-Yan Liu. Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/c0f168ce8900fa56e57789e2a2f2c9d0-Abstract.html}{In NeurIPS 2021}}.

    \bibitem{LingxiaoHuang2021} Lingxiao Huang, K Sudhir, Nisheeth Vishnoi. Coresets for Time Series Clustering. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/c115ba9e04ab27fbbb664f932112246d-Abstract.html}{In NeurIPS 2021}}.
    \bibitem{BinhTang2021} Binh Tang, David Matteson. Probabilistic Transformer For Time Series Analysis. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/c68bd9055776bf38d8fc43c0ed283678-Abstract.html}{In NeurIPS 2021}}.
    \bibitem{AnupSarma2021} Anup Sarma, Sonali Singh, Huaipan Jiang, Rui Zhang, Mahmut Kandemir, Chita Das. Structured in Space, Randomized in Time: Leveraging Dropout in RNNs for Efficient Training. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/cd81cfd0a3397761fac44ddbe5ec3349-Abstract.html}{In NeurIPS 2021}}.
    \bibitem{SebastianZeng2021} Sebastian Zeng, Florian Graf, Christoph Hofer, Roland Kwitt. Topological Attention for Time Series Forecasting. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/d062f3e278a1fbba2303ff5a22e8c75e-Abstract.html}{In NeurIPS 2021}}.
    \bibitem{ShuhaoCao2021} Shuhao Cao. Choose a Transformer: Fourier or Galerkin. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/d0921d442ee91b896ad95059d13df618-Abstract.html}{In NeurIPS 2021}}.

    \bibitem{EliaTurner2021} Elia Turner, Kabir Dabholkar, Omri Barak. Charting and Navigating the Space of Solutions for Recurrent Neural Networks. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/d530d454337fb09964237fecb4bea6ce-Abstract.html}{In NeurIPS 2021}}.
    \bibitem{ZimingZhang2021} Ziming Zhang, Yun Yue, Guojun Wu, Yanhua Li, Haichong Zhang. SBO-RNN: Reformulating Recurrent Neural Networks via Stochastic Bilevel Optimization. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/d87ca511e2a8593c8039ef732f5bffed-Abstract.html}{In NeurIPS 2021}}.
    \bibitem{QuentinRebjock2021} Quentin Rebjock, Baris Kurt, Tim Januschowski, Laurent Callot. Online false discovery rate control for anomaly detection in time series. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/def130d0b67eb38b7a8f4e7121ed432c-Abstract.html}{In NeurIPS 2021}}.
    \bibitem{YutongBai2021} Yutong Bai, Jieru Mei, Alan L. Yuille, Cihang Xie. Are Transformers more robust than CNNs? . {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/e19347e1c3ca0c0b97de5fb3b690855a-Abstract.html}{In NeurIPS 2021}}.
    \bibitem{StephenChung2021} Stephen Chung, Hava Siegelmann. Turing Completeness of Bounded-Precision Recurrent Neural Networks. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/ef452c63f81d0105dd4486f775adec81-Abstract.html}{In NeurIPS 2021}}.

    \bibitem{JannikKossen2021} Jannik Kossen, Neil Band, Clare Lyle, Aidan N. Gomez, Thomas Rainforth, Yarin Gal. Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/f1507aba9fc82ffa7cc7373c58f8a613-Abstract.html}{In NeurIPS 2021}}.
    \bibitem{TanNguyen2021} Tan Nguyen, Vai Suliafu, Stanley Osher, Long Chen, Bao Wang. FMMformer: Efficient and Flexible Transformer via Decomposed Near-field and Far-field Attention. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/f621585df244e9596dc70a39b579efb1-Abstract.html}{In NeurIPS 2021}}.
    \bibitem{FanKengSun2021} Fan-Keng Sun, Chris Lang, Duane Boning. Adjusting for Autocorrelated Errors in Neural Networks for Time Series. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/f8e6ba1db0f3c4054afec1684ba8fb26-Abstract.html}{In NeurIPS 2021}}.
    \bibitem{AbdulFatirAnsari2021} Abdul Fatir Ansari, Konstantinos Benidis, Richard Kurle, Ali Caner Turkmen, Harold Soh, Alexander J. Smola, Bernie Wang, Tim Januschowski. Deep Explicit Duration Switching Models for Time Series. {\addfontfeatures{Color=SteelBlue}\href{https://proceedings.neurips.cc/paper/2021/hash/fb4c835feb0a65cc39739320d7a51c02-Abstract.html}{In NeurIPS 2021}}.
\end{thebibliography}


% 奥付
\clearpage
\vspace*{16.7cm}

\begin{OKUDUKE}[title={NeurIPS 2021 にみる \, \textbf{\large 最近のニューラル系列モデルへの発見・工夫・理解}}]
YYYY年MM月DD日　初版発行 \\
YYYY年MM月DD日　第2版発行 \\
{
\renewcommand\arraystretch{0.9}
\begin{tabular}{p{4cm}rp{5.9cm}}
 &  &  \\
 & 著　者 & クッキー \\
 & 発行者 & クッキーの日記 \\
 & & https://cookie-box.hatenablog.com/ 
\end{tabular}
}
\end{OKUDUKE}
\thispagestyle{empty}


\end{document}
